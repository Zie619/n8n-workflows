#!/usr/bin/env python3
"""
N8N å·¥ä½œæµæ–‡æ¡£çš„ FastAPI æœåŠ¡å™¨
é«˜æ€§èƒ½ APIï¼Œå“åº”æ—¶é—´ä½äº 100msã€‚
"""

from fastapi import FastAPI, HTTPException, Query, BackgroundTasks, Request
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse, FileResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from pydantic import BaseModel, field_validator
from typing import Optional, List, Dict, Any
import json
import os
import asyncio
import re
import urllib.parse
from pathlib import Path
import uvicorn
import time
from collections import defaultdict

from workflow_db import WorkflowDatabase

# åˆå§‹åŒ–FastAPIåº”ç”¨
app = FastAPI(
    title="N8N å·¥ä½œæµæ–‡æ¡£ API",
    description="ç”¨äºæµè§ˆå’Œæœç´¢å·¥ä½œæµæ–‡æ¡£çš„å¿«é€ŸAPI",
    version="2.0.0"
)

# å®‰å…¨ï¼šé€Ÿç‡é™åˆ¶å­˜å‚¨
rate_limit_storage = defaultdict(list)
MAX_REQUESTS_PER_MINUTE = 60  # æ ¹æ®éœ€è¦é…ç½®

# æ·»åŠ ä¸­é—´ä»¶ä»¥æé«˜æ€§èƒ½
app.add_middleware(GZipMiddleware, minimum_size=1000)

# å®‰å…¨ï¼šæ­£ç¡®é…ç½®CORS - åœ¨ç”Ÿäº§ç¯å¢ƒä¸­é™åˆ¶æ¥æº
# å¯¹äºæœ¬åœ°å¼€å‘ï¼Œå¯ä»¥ä½¿ç”¨localhost
# å¯¹äºç”Ÿäº§ç¯å¢ƒï¼Œè¯·æ›¿æ¢ä¸ºæ‚¨çš„å®é™…åŸŸå
ALLOWED_ORIGINS = [
    "http://localhost:3000",
    "http://localhost:8000",
    "http://localhost:8080",
    "https://zie619.github.io",  # GitHub Pages
    "https://n8n-workflows-1-xxgm.onrender.com",  # Community deployment
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS,  # å®‰å…¨ä¿®å¤ï¼šé™åˆ¶æ¥æºåœ°å€
    allow_credentials=True,
    allow_methods=["GET", "POST"],  # å®‰å…¨ä¿®å¤ï¼šä»…å…è®¸éœ€è¦çš„æ–¹æ³•
    allow_headers=["Content-Type", "Authorization"],  # å®‰å…¨ä¿®å¤ï¼šé™åˆ¶è¯·æ±‚å¤´
)

# åˆå§‹åŒ–æ•°æ®åº“
db = WorkflowDatabase()

# å®‰å…¨ï¼šé€Ÿç‡é™åˆ¶è¾…åŠ©å‡½æ•°
def check_rate_limit(client_ip: str) -> bool:
    """æ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦è¶…å‡ºé€Ÿç‡é™åˆ¶ã€‚"""
    current_time = time.time()
    # Clean old entries
    rate_limit_storage[client_ip] = [
        timestamp for timestamp in rate_limit_storage[client_ip]
        if current_time - timestamp < 60
    ]
    # Check rate limit
    if len(rate_limit_storage[client_ip]) >= MAX_REQUESTS_PER_MINUTE:
        return False
    # Add current request
    rate_limit_storage[client_ip].append(current_time)
    return True

# å®‰å…¨ï¼šéªŒè¯å’Œæ¸…ç†æ–‡ä»¶åçš„è¾…åŠ©å‡½æ•°
def validate_filename(filename: str) -> bool:
    """
    éªŒè¯æ–‡ä»¶åä»¥é˜²æ­¢è·¯å¾„éå†æ”»å‡»ã€‚
    å¦‚æœæ–‡ä»¶åå®‰å…¨è¿”å›Trueï¼Œå¦åˆ™è¿”å›Falseã€‚
    """
    # å¤šæ¬¡è§£ç URLç¼–ç ä»¥æ•è·ç¼–ç çš„éå†å°è¯•
    decoded = filename
    for _ in range(3):  # æœ€å¤šè§£ç 3æ¬¡ä»¥æ•è·åµŒå¥—ç¼–ç 
        try:
            decoded = urllib.parse.unquote(decoded, errors='strict')
        except:
            return False  # æ— æ•ˆçš„ç¼–ç 

    # æ£€æŸ¥è·¯å¾„éå†æ¨¡å¼
    dangerous_patterns = [
        '..',  # çˆ¶ç›®å½•
        '..\\',  # Windowsçˆ¶ç›®å½•
        '../',  # Unixçˆ¶ç›®å½•
        '\\',  # åæ–œæ  (Windowsè·¯å¾„åˆ†éš”ç¬¦)
        '/',  # æ­£æ–œæ  (Unixè·¯å¾„åˆ†éš”ç¬¦)
        '\x00',  # ç©ºå­—èŠ‚
        '\n', '\r',  # æ¢è¡Œç¬¦
        '~',  # ä¸»ç›®å½•
        ':',  # é©±åŠ¨å™¨å·æˆ–æµ (Windows)
        '|', '<', '>',  # Shellé‡å®šå‘
        '*', '?',  # é€šé…ç¬¦
        '$',  # å˜é‡æ‰©å±•
        ';', '&',  # å‘½ä»¤åˆ†éš”ç¬¦
    ]

    for pattern in dangerous_patterns:
        if pattern in decoded:
            return False

    # æ£€æŸ¥ç»å¯¹è·¯å¾„
    if decoded.startswith('/') or decoded.startswith('\\'):
        return False

    # æ£€æŸ¥Windowsé©±åŠ¨å™¨å·
    if len(decoded) >= 2 and decoded[1] == ':':
        return False

    # ä»…å…è®¸å­—æ¯æ•°å­—ã€ç ´æŠ˜å·ã€ä¸‹åˆ’çº¿å’Œ.jsonæ‰©å±•å
    if not re.match(r'^[a-zA-Z0-9_\-]+\.json$', decoded):
        return False

    # é¢å¤–æ£€æŸ¥ï¼šæ–‡ä»¶ååº”ä»¥.jsonç»“å°¾
    if not decoded.endswith('.json'):
        return False

    return True

# å¯åŠ¨å‡½æ•°ï¼Œç”¨äºéªŒè¯æ•°æ®åº“
@app.on_event("startup")
async def startup_event():
    """åœ¨å¯åŠ¨æ—¶éªŒè¯æ•°æ®åº“è¿æ¥ã€‚"""
    try:
        stats = db.get_stats()
        if stats['total'] == 0:
            print("âš ï¸  è­¦å‘Šï¼šæ•°æ®åº“ä¸­æœªæ‰¾åˆ°å·¥ä½œæµã€‚è¯·å…ˆè¿è¡Œç´¢å¼•ã€‚")
        else:
            print(f"âœ… æ•°æ®åº“å·²è¿æ¥ï¼šå·²ç´¢å¼• {stats['total']} ä¸ªå·¥ä½œæµ")
    except Exception as e:
        print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥ï¼š{e}")
        raise

# å“åº”æ¨¡å‹
class WorkflowSummary(BaseModel):
    id: Optional[int] = None
    filename: str
    name: str
    active: bool
    description: str = ""
    trigger_type: str = "Manual"
    complexity: str = "low"
    node_count: int = 0
    integrations: List[str] = []
    tags: List[str] = []
    created_at: Optional[str] = None
    updated_at: Optional[str] = None
    
    class Config:
        # å…è®¸å°†æ•´æ•°è½¬æ¢ä¸ºå¸ƒå°”å€¼ï¼Œç”¨äºactiveå­—æ®µ
        validate_assignment = True
        
    @field_validator('active', mode='before')
    @classmethod
    def convert_active(cls, v):
        if isinstance(v, int):
            return bool(v)
        return v
    

class SearchResponse(BaseModel):
    """
    æœç´¢å·¥ä½œæµçš„å“åº”æ¨¡å‹
    
    ç”¨äºè¿”å›åˆ†é¡µçš„å·¥ä½œæµæœç´¢ç»“æœå’Œç›¸å…³å…ƒæ•°æ®
    """
    workflows: List[WorkflowSummary]  # å·¥ä½œæµåˆ—è¡¨ï¼Œæ¯ä¸ªé¡¹åŒ…å«å·¥ä½œæµçš„è¯¦ç»†ä¿¡æ¯
    total: int  # åŒ¹é…æœç´¢æ¡ä»¶çš„å·¥ä½œæµæ€»æ•°
    page: int  # å½“å‰é¡µç 
    per_page: int  # æ¯é¡µæ˜¾ç¤ºçš„å·¥ä½œæµæ•°é‡
    pages: int  # æ€»é¡µæ•°
    query: str  # æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
    filters: Dict[str, Any]  # åº”ç”¨çš„è¿‡æ»¤æ¡ä»¶

class StatsResponse(BaseModel):
    """
    å·¥ä½œæµç»Ÿè®¡ä¿¡æ¯çš„å“åº”æ¨¡å‹
    
    ç”¨äºè¿”å›å·¥ä½œæµæ•°æ®åº“çš„ç»Ÿè®¡ä¿¡æ¯å’Œæ±‡æ€»æ•°æ®
    """
    total: int  # å·¥ä½œæµæ€»æ•°
    active: int  # æ´»è·ƒå·¥ä½œæµæ•°é‡
    inactive: int  # éæ´»è·ƒå·¥ä½œæµæ•°é‡
    triggers: Dict[str, int]  # æŒ‰è§¦å‘å™¨ç±»å‹åˆ†ç»„çš„å·¥ä½œæµè®¡æ•°
    complexity: Dict[str, int]  # æŒ‰å¤æ‚åº¦åˆ†ç»„çš„å·¥ä½œæµè®¡æ•°
    total_nodes: int  # æ‰€æœ‰å·¥ä½œæµçš„èŠ‚ç‚¹æ€»æ•°
    unique_integrations: int  # å”¯ä¸€é›†æˆçš„æ•°é‡
    last_indexed: str  # æœ€åä¸€æ¬¡ç´¢å¼•çš„æ—¶é—´æˆ³

@app.get("/")
async def root():
    """æä¾›ä¸»æ–‡æ¡£é¡µé¢ã€‚"""
    static_dir = Path("static")
    index_file = static_dir / "index.html"
    if not index_file.exists():
        return HTMLResponse("""
        <html><body>
        <h1>éœ€è¦è®¾ç½®</h1>
        <p>æœªæ‰¾åˆ°é™æ€æ–‡ä»¶ã€‚è¯·ç¡®ä¿é™æ€ç›®å½•å­˜åœ¨ä¸”åŒ…å« index.html</p>
        <p>å½“å‰ç›®å½•ï¼š""" + str(Path.cwd()) + """
        </body></html>
        """)
    return FileResponse(str(index_file))

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹ã€‚"""
    return {"status": "healthy", "message": "N8N å·¥ä½œæµ API æ­£åœ¨è¿è¡Œ"}

@app.get("/api/stats", response_model=StatsResponse)
async def get_stats():
    """è·å–å·¥ä½œæµæ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯ã€‚"""
    try:
        stats = db.get_stats()
        return StatsResponse(**stats)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")

@app.get("/api/workflows", response_model=SearchResponse)
async def search_workflows(
    q: str = Query("", description="æœç´¢æŸ¥è¯¢"),
    trigger: str = Query("all", description="æŒ‰è§¦å‘å™¨ç±»å‹è¿‡æ»¤"),
    complexity: str = Query("all", description="æŒ‰å¤æ‚åº¦è¿‡æ»¤"),
    active_only: bool = Query(False, description="ä»…æ˜¾ç¤ºæ´»è·ƒå·¥ä½œæµ"),
    page: int = Query(1, ge=1, description="é¡µç "),
    per_page: int = Query(20, ge=1, le=100, description="æ¯é¡µé¡¹æ•°")
):
    """ä½¿ç”¨åˆ†é¡µæœç´¢å’Œè¿‡æ»¤å·¥ä½œæµã€‚"""
    try:
        offset = (page - 1) * per_page
        
        workflows, total = db.search_workflows(
            query=q,
            trigger_filter=trigger,
            complexity_filter=complexity,
            active_only=active_only,
            limit=per_page,
            offset=offset
        )
        
        # Convert to Pydantic models with error handling
        workflow_summaries = []
        for workflow in workflows:
            try:
                # Remove extra fields that aren't in the model
                clean_workflow = {
                    'id': workflow.get('id'),
                    'filename': workflow.get('filename', ''),
                    'name': workflow.get('name', ''),
                    'active': workflow.get('active', False),
                    'description': workflow.get('description', ''),
                    'trigger_type': workflow.get('trigger_type', 'Manual'),
                    'complexity': workflow.get('complexity', 'low'),
                    'node_count': workflow.get('node_count', 0),
                    'integrations': workflow.get('integrations', []),
                    'tags': workflow.get('tags', []),
                    'created_at': workflow.get('created_at'),
                    'updated_at': workflow.get('updated_at')
                }
                workflow_summaries.append(WorkflowSummary(**clean_workflow))
            except Exception as e:
                print(f"è½¬æ¢å·¥ä½œæµ {workflow.get('filename', 'unknown')} æ—¶å‡ºé”™ï¼š{e}")
                # Continue with other workflows instead of failing completely
                continue
        
        pages = (total + per_page - 1) // per_page  # Ceiling division
        
        return SearchResponse(
            workflows=workflow_summaries,
            total=total,
            page=page,
            per_page=per_page,
            pages=pages,
            query=q,
            filters={
                "trigger": trigger,
                "complexity": complexity,
                "active_only": active_only
            }
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æœç´¢å·¥ä½œæµå¤±è´¥: {str(e)}")

@app.get("/api/workflows/{filename}")
async def get_workflow_detail(filename: str, request: Request):
    """è·å–å·¥ä½œæµè¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬åŸå§‹JSONã€‚"""
    try:
        # å®‰å…¨ï¼šéªŒè¯æ–‡ä»¶åä»¥é˜²æ­¢è·¯å¾„éå†
        if not validate_filename(filename):
            print(f"å®‰å…¨ï¼šå·²é˜»æ­¢å¯¹æ–‡ä»¶åçš„è·¯å¾„éå†å°è¯•ï¼š{filename}")
            raise HTTPException(status_code=400, detail="æ— æ•ˆçš„æ–‡ä»¶åæ ¼å¼")

        # å®‰å…¨ï¼šé€Ÿç‡é™åˆ¶
        client_ip = request.client.host if request.client else "unknown"
        if not check_rate_limit(client_ip):
            raise HTTPException(status_code=429, detail="è¯·æ±‚é¢‘ç‡è¿‡é«˜ï¼Œè¯·ç¨åå†è¯•ã€‚")

        # ä»æ•°æ®åº“è·å–å·¥ä½œæµå…ƒæ•°æ®
        workflows, _ = db.search_workflows(f'filename:"{filename}"', limit=1)
        if not workflows:
            raise HTTPException(status_code=404, detail="æ•°æ®åº“ä¸­æœªæ‰¾åˆ°å·¥ä½œæµ")

        workflow_meta = workflows[0]

        # ä»æ–‡ä»¶åŠ è½½åŸå§‹JSONï¼ˆåŒ…å«å®‰å…¨æ£€æŸ¥ï¼‰
        workflows_path = Path('workflows').resolve()

        # å®‰å…¨åœ°æŸ¥æ‰¾æ–‡ä»¶
        matching_file = None
        for subdir in workflows_path.iterdir():
            if subdir.is_dir():
                target_file = subdir / filename
                if target_file.exists() and target_file.is_file():
                    # éªŒè¯æ–‡ä»¶ç¡®å®åœ¨å·¥ä½œæµç›®å½•å†…
                    try:
                        target_file.resolve().relative_to(workflows_path)
                        matching_file = target_file
                        break
                    except ValueError:
                        print(f"å®‰å…¨ï¼šå·²é˜»æ­¢è®¿é—®å·¥ä½œæµç›®å½•å¤–çš„æ–‡ä»¶ï¼š{target_file}")
                        continue

        if not matching_file:
            print(f"è­¦å‘Šï¼šåœ¨å·¥ä½œæµç›®å½•ä¸­æœªæ‰¾åˆ°æ–‡ä»¶ {filename}")
            raise HTTPException(status_code=404, detail=f"æ–‡ä»¶ç³»ç»Ÿä¸­æœªæ‰¾åˆ°å·¥ä½œæµæ–‡ä»¶ '{filename}'")

        with open(matching_file, 'r', encoding='utf-8') as f:
            raw_json = json.load(f)

        return {
            "metadata": workflow_meta,
            "raw_json": raw_json
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åŠ è½½å·¥ä½œæµå¤±è´¥: {str(e)}")

@app.get("/api/workflows/{filename}/download")
async def download_workflow(filename: str, request: Request):
    """ä¸‹è½½å·¥ä½œæµJSONæ–‡ä»¶ï¼ˆåŒ…å«å®‰å…¨éªŒè¯ï¼‰ã€‚"""
    try:
        # Security: Validate filename to prevent path traversal
        if not validate_filename(filename):
            print(f"å®‰å…¨ï¼šå·²é˜»æ­¢å¯¹æ–‡ä»¶åçš„è·¯å¾„éå†å°è¯•ï¼š{filename}")
            raise HTTPException(status_code=400, detail="æ— æ•ˆçš„æ–‡ä»¶åæ ¼å¼")

        # Security: Rate limiting
        client_ip = request.client.host if request.client else "unknown"
        if not check_rate_limit(client_ip):
            raise HTTPException(status_code=429, detail="è¯·æ±‚é¢‘ç‡è¿‡é«˜ï¼Œè¯·ç¨åå†è¯•ã€‚")

        # ä»…åœ¨å·¥ä½œæµç›®å½•å†…æœç´¢
        workflows_path = Path('workflows').resolve()  # Get absolute path

        # Find the file safely
        json_files = []
        for subdir in workflows_path.iterdir():
            if subdir.is_dir():
                target_file = subdir / filename
                if target_file.exists() and target_file.is_file():
                    # éªŒè¯æ–‡ä»¶ç¡®å®åœ¨å·¥ä½œæµç›®å½•å†…ï¼ˆçºµæ·±é˜²å¾¡ï¼‰
                    try:
                        target_file.resolve().relative_to(workflows_path)
                        json_files.append(target_file)
                    except ValueError:
                        # æ–‡ä»¶åœ¨å·¥ä½œæµç›®å½•å¤–
                        print(f"å®‰å…¨ï¼šå·²é˜»æ­¢è®¿é—®å·¥ä½œæµç›®å½•å¤–çš„æ–‡ä»¶ï¼š{target_file}")
                        continue

        if not json_files:
            print(f"åœ¨å·¥ä½œæµç›®å½•ä¸­æœªæ‰¾åˆ°æ–‡ä»¶ {filename}")
            raise HTTPException(status_code=404, detail=f"æœªæ‰¾åˆ°å·¥ä½œæµæ–‡ä»¶ '{filename}'")

        file_path = json_files[0]

        # æœ€ç»ˆå®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿æ–‡ä»¶åœ¨å·¥ä½œæµç›®å½•å†…
        try:
            file_path.resolve().relative_to(workflows_path)
        except ValueError:
            print(f"å®‰å…¨ï¼šå·²é˜»æ­¢æœ€ç»ˆè®¿é—®å·¥ä½œæµç›®å½•å¤–æ–‡ä»¶çš„å°è¯•ï¼š{file_path}")
            raise HTTPException(status_code=403, detail="è®¿é—®è¢«æ‹’ç»")

        return FileResponse(
            str(file_path),
            media_type="application/json",
            filename=filename
        )
    except HTTPException:
        raise
    except Exception as e:
        print(f"ä¸‹è½½å·¥ä½œæµ {filename} æ—¶å‡ºé”™ï¼š{str(e)}")
        raise HTTPException(status_code=500, detail=f"ä¸‹è½½å·¥ä½œæµå¤±è´¥: {str(e)}")

@app.get("/api/workflows/{filename}/diagram")
async def get_workflow_diagram(filename: str, request: Request):
    """è·å–ç”¨äºå·¥ä½œæµå¯è§†åŒ–çš„Mermaidå›¾è¡¨ä»£ç ã€‚"""
    try:
        # Security: Validate filename to prevent path traversal
        if not validate_filename(filename):
            print(f"å®‰å…¨ï¼šå·²é˜»æ­¢å¯¹æ–‡ä»¶åçš„è·¯å¾„éå†å°è¯•ï¼š{filename}")
            raise HTTPException(status_code=400, detail="æ— æ•ˆçš„æ–‡ä»¶åæ ¼å¼")

        # Security: Rate limiting
        client_ip = request.client.host if request.client else "unknown"
        if not check_rate_limit(client_ip):
            raise HTTPException(status_code=429, detail="è¯·æ±‚é¢‘ç‡è¿‡é«˜ï¼Œè¯·ç¨åå†è¯•ã€‚")

        # Only search within the workflows directory
        workflows_path = Path('workflows').resolve()

        # Find the file safely
        matching_file = None
        for subdir in workflows_path.iterdir():
            if subdir.is_dir():
                target_file = subdir / filename
                if target_file.exists() and target_file.is_file():
                    # Verify the file is actually within workflows directory
                    try:
                        target_file.resolve().relative_to(workflows_path)
                        matching_file = target_file
                        break
                    except ValueError:
                        print(f"å®‰å…¨ï¼šå·²é˜»æ­¢è®¿é—®å·¥ä½œæµç›®å½•å¤–çš„æ–‡ä»¶ï¼š{target_file}")
                        continue

        if not matching_file:
            print(f"è­¦å‘Šï¼šåœ¨å·¥ä½œæµç›®å½•ä¸­æœªæ‰¾åˆ°æ–‡ä»¶ {filename}")
            raise HTTPException(status_code=404, detail=f"Workflow file '{filename}' not found on filesystem")

        with open(matching_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        nodes = data.get('nodes', [])
        connections = data.get('connections', {})

        # ç”ŸæˆMermaidå›¾è¡¨
        diagram = generate_mermaid_diagram(nodes, connections)

        return {"diagram": diagram}
    except HTTPException:
        raise
    except json.JSONDecodeError as e:
        print(f"è§£æ {filename} ä¸­çš„JSONæ—¶å‡ºé”™ï¼š{str(e)}")
        raise HTTPException(status_code=400, detail=f"å·¥ä½œæµæ–‡ä»¶ä¸­çš„JSONæ— æ•ˆ: {str(e)}")
    except Exception as e:
        print(f"ä¸º {filename} ç”Ÿæˆå›¾è¡¨æ—¶å‡ºé”™ï¼š{str(e)}")
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆå›¾è¡¨å¤±è´¥: {str(e)}")

def generate_mermaid_diagram(nodes: List[Dict], connections: Dict) -> str:
    """ä»å·¥ä½œæµèŠ‚ç‚¹å’Œè¿æ¥ç”ŸæˆMermaid.jsæµç¨‹å›¾ä»£ç ã€‚"""
    if not nodes:
        return "graph TD\n  EmptyWorkflow[No nodes found in workflow]"
    
    # åˆ›å»ºèŠ‚ç‚¹åç§°æ˜ å°„ä»¥ç¡®ä¿æœ‰æ•ˆçš„mermaid ID
    mermaid_ids = {}
    for i, node in enumerate(nodes):
        node_id = f"node{i}"
        node_name = node.get('name', f'Node {i}')
        mermaid_ids[node_name] = node_id
    
    # å¼€å§‹æ„å»ºmermaidå›¾è¡¨
    mermaid_code = ["graph TD"]
    
    # æ·»åŠ å¸¦æ ·å¼çš„èŠ‚ç‚¹
    for node in nodes:
        node_name = node.get('name', 'Unnamed')
        node_id = mermaid_ids[node_name]
        node_type = node.get('type', '').replace('n8n-nodes-base.', '')
        
        # æ ¹æ®ç±»å‹ç¡®å®šèŠ‚ç‚¹æ ·å¼
        style = ""
        if any(x in node_type.lower() for x in ['trigger', 'webhook', 'cron']):
            style = "fill:#b3e0ff,stroke:#0066cc"  # Blue for triggers
        elif any(x in node_type.lower() for x in ['if', 'switch']):
            style = "fill:#ffffb3,stroke:#e6e600"  # Yellow for conditional nodes
        elif any(x in node_type.lower() for x in ['function', 'code']):
            style = "fill:#d9b3ff,stroke:#6600cc"  # Purple for code nodes
        elif 'error' in node_type.lower():
            style = "fill:#ffb3b3,stroke:#cc0000"  # Red for error handlers
        else:
            style = "fill:#d9d9d9,stroke:#666666"  # Gray for other nodes
        
        # Add node with label (escaping special characters)
        clean_name = node_name.replace('"', "'")
        clean_type = node_type.replace('"', "'")
        label = f"{clean_name}<br>({clean_type})"
        mermaid_code.append(f"  {node_id}[\"{label}\"]")
        mermaid_code.append(f"  style {node_id} {style}")
    
    # æ·»åŠ èŠ‚ç‚¹ä¹‹é—´çš„è¿æ¥
    for source_name, source_connections in connections.items():
        if source_name not in mermaid_ids:
            continue
        
        if isinstance(source_connections, dict) and 'main' in source_connections:
            main_connections = source_connections['main']
            
            for i, output_connections in enumerate(main_connections):
                if not isinstance(output_connections, list):
                    continue
                    
                for connection in output_connections:
                    if not isinstance(connection, dict) or 'node' not in connection:
                        continue
                        
                    target_name = connection['node']
                    if target_name not in mermaid_ids:
                        continue
                        
                    # Add arrow with output index if multiple outputs
                    label = f" -->|{i}| " if len(main_connections) > 1 else " --> "
                    mermaid_code.append(f"  {mermaid_ids[source_name]}{label}{mermaid_ids[target_name]}")
    
    # Format the final mermaid diagram code
    return "\n".join(mermaid_code)

@app.post("/api/reindex")
async def reindex_workflows(
    background_tasks: BackgroundTasks,
    request: Request,
    force: bool = False,
    admin_token: Optional[str] = Query(None, description="ç®¡ç†å‘˜è®¤è¯ä»¤ç‰Œ")
):
    """åœ¨åå°è§¦å‘å·¥ä½œæµé‡æ–°ç´¢å¼•ï¼ˆéœ€è¦è®¤è¯ï¼‰ã€‚"""
    # Security: Rate limiting
    client_ip = request.client.host if request.client else "unknown"
    if not check_rate_limit(client_ip):
        raise HTTPException(status_code=429, detail="è¯·æ±‚é¢‘ç‡è¿‡é«˜ï¼Œè¯·ç¨åå†è¯•ã€‚")

    # å®‰å…¨ï¼šåŸºæœ¬è®¤è¯æ£€æŸ¥
    # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œä½¿ç”¨é€‚å½“çš„è®¤è¯ï¼ˆJWTã€OAuthç­‰ï¼‰
# ç›®å‰ï¼Œæ£€æŸ¥ç¯å¢ƒå˜é‡æˆ–ç¦ç”¨ç«¯ç‚¹
    import os
    expected_token = os.environ.get("ADMIN_TOKEN", None)

    if not expected_token:
        # å¦‚æœæœªé…ç½®ä»¤ç‰Œï¼Œåˆ™ä¸ºå®‰å…¨èµ·è§ç¦ç”¨è¯¥ç«¯ç‚¹
        raise HTTPException(
            status_code=503,
            detail="é‡æ–°ç´¢å¼•ç«¯ç‚¹å·²ç¦ç”¨ã€‚è®¾ç½® ADMIN_TOKEN ç¯å¢ƒå˜é‡ä»¥å¯ç”¨ã€‚"
        )

    if admin_token != expected_token:
        print(f"å®‰å…¨ï¼šæ¥è‡ª {client_ip} çš„æœªæˆæƒé‡æ–°ç´¢å¼•å°è¯•")
        raise HTTPException(status_code=401, detail="æ— æ•ˆçš„è®¤è¯ä»¤ç‰Œ")

    def run_indexing():
        try:
            db.index_all_workflows(force_reindex=force)
            print(f"é‡æ–°ç´¢å¼•æˆåŠŸå®Œæˆï¼ˆç”± {client_ip} è¯·æ±‚ï¼‰")
        except Exception as e:
            print(f"é‡æ–°ç´¢å¼•æœŸé—´å‡ºé”™ï¼š{e}")

    background_tasks.add_task(run_indexing)
    return {"message": "åå°é‡æ–°ç´¢å¼•å·²å¼€å§‹", "requested_by": client_ip}

@app.get("/api/integrations")
async def get_integrations():
    """è·å–æ‰€æœ‰å”¯ä¸€é›†æˆçš„åˆ—è¡¨ã€‚"""
    try:
        stats = db.get_stats()
        # ç›®å‰ï¼Œè¿”å›åŸºæœ¬ä¿¡æ¯ã€‚å¯ä»¥å¢å¼ºä»¥è¿”å›è¯¦ç»†çš„é›†æˆç»Ÿè®¡ä¿¡æ¯
        return {"integrations": [], "count": stats['unique_integrations']}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–é›†æˆå¤±è´¥: {str(e)}")

@app.get("/api/categories")
async def get_categories():
    """è·å–ç”¨äºè¿‡æ»¤çš„å¯ç”¨å·¥ä½œæµç±»åˆ«ã€‚"""
    try:
        # å°è¯•ä»ç”Ÿæˆçš„å”¯ä¸€ç±»åˆ«æ–‡ä»¶åŠ è½½
        categories_file = Path("context/unique_categories.json")
        if categories_file.exists():
            with open(categories_file, 'r', encoding='utf-8') as f:
                categories = json.load(f)
            return {"categories": categories}
        else:
            # å¤‡é€‰æ–¹æ¡ˆï¼šä»search_categories.jsonæå–ç±»åˆ«
            search_categories_file = Path("context/search_categories.json")
            if search_categories_file.exists():
                with open(search_categories_file, 'r', encoding='utf-8') as f:
                    search_data = json.load(f)
                
                unique_categories = set()
                for item in search_data:
                    if item.get('category'):
                        unique_categories.add(item['category'])
                    else:
                        unique_categories.add('æœªåˆ†ç±»')
                
                categories = sorted(list(unique_categories))
                return {"categories": categories}
            else:
                # æœ€åæ‰‹æ®µï¼šè¿”å›åŸºæœ¬ç±»åˆ«
                return {"categories": ["æœªåˆ†ç±»"]}
                
    except Exception as e:
        print(f"åŠ è½½ç±»åˆ«æ—¶å‡ºé”™ï¼š{e}")
        raise HTTPException(status_code=500, detail=f"è·å–ç±»åˆ«å¤±è´¥: {str(e)}")

@app.get("/api/category-mappings")
async def get_category_mappings():
    """è·å–æ–‡ä»¶ååˆ°ç±»åˆ«çš„æ˜ å°„ï¼Œç”¨äºå®¢æˆ·ç«¯è¿‡æ»¤ã€‚"""
    try:
        search_categories_file = Path("context/search_categories.json")
        if not search_categories_file.exists():
            return {"mappings": {}}
        
        with open(search_categories_file, 'r', encoding='utf-8') as f:
            search_data = json.load(f)
        
        # è½¬æ¢ä¸ºç®€å•çš„æ–‡ä»¶å -> ç±»åˆ«æ˜ å°„
        mappings = {}
        for item in search_data:
            filename = item.get('filename')
            category = item.get('category') or 'æœªåˆ†ç±»'
            if filename:
                mappings[filename] = category
        
        return {"mappings": mappings}
        
    except Exception as e:
        print(f"åŠ è½½ç±»åˆ«æ˜ å°„æ—¶å‡ºé”™ï¼š{e}")
        raise HTTPException(status_code=500, detail=f"è·å–ç±»åˆ«æ˜ å°„å¤±è´¥: {str(e)}")

@app.get("/api/workflows/category/{category}", response_model=SearchResponse)
async def search_workflows_by_category(
    category: str,
    page: int = Query(1, ge=1, description="é¡µç "),
    per_page: int = Query(20, ge=1, le=100, description="æ¯é¡µé¡¹æ•°")
):
    """æŒ‰æœåŠ¡ç±»åˆ«ï¼ˆæ¶ˆæ¯ä¼ é€’ã€æ•°æ®åº“ã€ai_mlç­‰ï¼‰æœç´¢å·¥ä½œæµã€‚"""
    try:
        offset = (page - 1) * per_page
        
        workflows, total = db.search_by_category(
            category=category,
            limit=per_page,
            offset=offset
        )
        
        # Convert to Pydantic models with error handling
        workflow_summaries = []
        for workflow in workflows:
            try:
                clean_workflow = {
                    'id': workflow.get('id'),
                    'filename': workflow.get('filename', ''),
                    'name': workflow.get('name', ''),
                    'active': workflow.get('active', False),
                    'description': workflow.get('description', ''),
                    'trigger_type': workflow.get('trigger_type', 'Manual'),
                    'complexity': workflow.get('complexity', 'low'),
                    'node_count': workflow.get('node_count', 0),
                    'integrations': workflow.get('integrations', []),
                    'tags': workflow.get('tags', []),
                    'created_at': workflow.get('created_at'),
                    'updated_at': workflow.get('updated_at')
                }
                workflow_summaries.append(WorkflowSummary(**clean_workflow))
            except Exception as e:
                print(f"è½¬æ¢å·¥ä½œæµ {workflow.get('filename', 'unknown')} æ—¶å‡ºé”™ï¼š{e}")
                continue
        
        pages = (total + per_page - 1) // per_page
        
        return SearchResponse(
            workflows=workflow_summaries,
            total=total,
            page=page,
            per_page=per_page,
            pages=pages,
            query=f"category:{category}",
            filters={"category": category}
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æŒ‰ç±»åˆ«æœç´¢å¤±è´¥: {str(e)}")

# è‡ªå®šä¹‰å¼‚å¸¸å¤„ç†å™¨ï¼Œæä¾›æ›´å¥½çš„é”™è¯¯å“åº”
@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    return JSONResponse(
        status_code=500,
        content={"detail": f"å†…éƒ¨æœåŠ¡å™¨é”™è¯¯: {str(exc)}"}
    )

# åœ¨å®šä¹‰æ‰€æœ‰è·¯ç”±åæŒ‚è½½é™æ€æ–‡ä»¶
static_dir = Path("static")
if static_dir.exists():
    app.mount("/static", StaticFiles(directory="static"), name="static")
    print(f"âœ… é™æ€æ–‡ä»¶å·²ä» {static_dir.absolute()} æŒ‚è½½")
else:
    print(f"âŒ è­¦å‘Šï¼šåœ¨ {static_dir.absolute()} æœªæ‰¾åˆ°é™æ€ç›®å½•")

def create_static_directory():
    """å¦‚æœé™æ€ç›®å½•ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºå®ƒã€‚"""
    static_dir = Path("static")
    static_dir.mkdir(exist_ok=True)
    return static_dir

def run_server(host: str = "127.0.0.1", port: int = 8000, reload: bool = False):
    """è¿è¡ŒFastAPIæœåŠ¡å™¨ã€‚"""
    # ç¡®ä¿é™æ€ç›®å½•å­˜åœ¨
    create_static_directory()
    
    # è°ƒè¯•ï¼šæ£€æŸ¥æ•°æ®åº“è¿æ¥
    try:
        stats = db.get_stats()
        print(f"âœ… æ•°æ®åº“å·²è¿æ¥ï¼šæ‰¾åˆ° {stats['total']} ä¸ªå·¥ä½œæµ")
        if stats['total'] == 0:
            print("ğŸ”„ æ•°æ®åº“ä¸ºç©ºã€‚æ­£åœ¨ç´¢å¼•å·¥ä½œæµ...")
            db.index_all_workflows()
            stats = db.get_stats()
    except Exception as e:
        print(f"âŒ æ•°æ®åº“é”™è¯¯ï¼š{e}")
        print("ğŸ”„ æ­£åœ¨å°è¯•åˆ›å»ºå’Œç´¢å¼•æ•°æ®åº“...")
        try:
            db.index_all_workflows()
            stats = db.get_stats()
            print(f"âœ… æ•°æ®åº“å·²åˆ›å»ºï¼šå·²ç´¢å¼• {stats['total']} ä¸ªå·¥ä½œæµ")
        except Exception as e2:
            print(f"âŒ åˆ›å»ºæ•°æ®åº“å¤±è´¥ï¼š{e2}")
            stats = {'total': 0}
    
    # è°ƒè¯•ï¼šæ£€æŸ¥é™æ€æ–‡ä»¶
    static_path = Path("static")
    if static_path.exists():
        files = list(static_path.glob("*"))
        print(f"âœ… æ‰¾åˆ°é™æ€æ–‡ä»¶ï¼š{[f.name for f in files]}")
    else:
        print(f"âŒ åœ¨ {static_path.absolute()} æœªæ‰¾åˆ°é™æ€ç›®å½•")
    
    print(f"ğŸš€ æ­£åœ¨å¯åŠ¨ N8N å·¥ä½œæµæ–‡æ¡£ API")
    print(f"ğŸ“Š æ•°æ®åº“åŒ…å« {stats['total']} ä¸ªå·¥ä½œæµ")
    print(f"ğŸŒ æœåŠ¡å™¨å°†åœ¨ä»¥ä¸‹åœ°å€å¯ç”¨ï¼šhttp://{host}:{port}")
    print(f"ğŸ“ é™æ€æ–‡ä»¶ä½ç½®ï¼šhttp://{host}:{port}/static/")
    
    uvicorn.run(
        "api_server:app",
        host=host,
        port=port,
        reload=reload,
        access_log=True,  # Enable access logs for debugging
        log_level="info"
    )

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='N8N å·¥ä½œæµæ–‡æ¡£ API æœåŠ¡å™¨')
    parser.add_argument('--host', default='127.0.0.1', help='ç»‘å®šçš„ä¸»æœºåœ°å€')
    parser.add_argument('--port', type=int, default=8000, help='ç»‘å®šçš„ç«¯å£å·')
    parser.add_argument('--reload', action='store_true', help='ä¸ºå¼€å‘ç¯å¢ƒå¯ç”¨è‡ªåŠ¨é‡è½½åŠŸèƒ½')
    
    args = parser.parse_args()
    
    run_server(host=args.host, port=args.port, reload=args.reload)